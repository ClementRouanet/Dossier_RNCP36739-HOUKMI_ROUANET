{
	"jobConfig": {
		"name": "customer_job",
		"description": "",
		"role": "arn:aws:iam::421189326716:role/glue_grp6",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.8X",
		"numberOfWorkers": 10,
		"maxCapacity": 80,
		"jobRunQueuingEnabled": null,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "customer_job.py",
		"scriptLocation": "s3://aws-glue-assets-421189326716-us-east-1/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-07-05T13:56:34.227Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-421189326716-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-421189326716-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import monotonically_increasing_id\n\n# Récupération des arguments nécessaires\nargs = getResolvedOptions(sys.argv, ['TempDir', 'JOB_NAME'])\n\n# Initialisation des contextes Spark et Glue\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Lecture des données JSON depuis le catalogue Glue\nread_s3_customers_json = glueContext.create_dynamic_frame.from_catalog(\n    database=\"raw_data_json\",\n    table_name=\"customers_json\",\n    transformation_ctx=\"read_s3_customers_json\"\n)\n\n# Application des mappings\napply_mapping = ApplyMapping.apply(\n    frame=read_s3_customers_json,\n    mappings=[\n        (\"customer_id\", \"string\", \"customer_id\", \"string\"),\n        (\"customer_unique_id\", \"string\", \"customer_unique_id\", \"string\"),\n        (\"customer_address.customer_zip_code\", \"int\", \"customer_zip_code\", \"int\"),\n        (\"customer_address.customer_city\", \"string\", \"customer_city\", \"string\"),\n        (\"customer_address.customer_state\", \"string\", \"customer_state\", \"string\")\n    ],\n    transformation_ctx=\"apply_mapping\"\n)\n\n# Convertir en DataFrame Spark\ndf = apply_mapping.toDF()\n\n# Ajouter un identifiant unique pour chaque ligne pour faciliter le découpage en lots\ndf = df.withColumn(\"id\", monotonically_increasing_id())\n\n# Définir la taille des lots\nbatch_size = 500\nnum_batches = df.count() // batch_size + (1 if df.count() % batch_size != 0 else 0)\n\n# Traitement des lots\nfor i in range(num_batches):\n    # Sélectionner le lot de lignes\n    batch_df = df.filter((df.id >= i * batch_size) & (df.id < (i + 1) * batch_size)).drop(\"id\")\n    \n    # Afficher les logs pour suivre les lots traités\n    print(f\"Traitement du lot {i+1} sur {num_batches}\")\n\n    # Convertir le lot en DynamicFrame\n    batch_dyf = DynamicFrame.fromDF(batch_df, glueContext, f\"batch_dyf_{i}\")\n    \n    # Écriture dans DynamoDB\n    glueContext.write_dynamic_frame.from_options(\n        frame=batch_dyf,\n        connection_type=\"dynamodb\",\n        connection_options={\n            \"dynamodb.output.tableName\": \"customers\",\n            \"dynamodb.throughput.write.percent\": \"1.0\",\n            \"dynamodb.throughput.read.percent\": \"1.0\",\n            \"dynamodb.throughput.write.units\": \"5\",\n            \"dynamodb.throughput.read.units\": \"5\",\n            \"dynamodb.billing.mode\": \"PROVISIONED\",\n            \"dynamodb.input.format\": \"json\",\n            \"dynamodb.output.mode\": \"OVERWRITE\"\n            #\"dynamodb.throughput.write.autoscaling.mode\": \"DISABLED\"\n        }\n    )\n\n# Commit du job\njob.commit()\n"
}