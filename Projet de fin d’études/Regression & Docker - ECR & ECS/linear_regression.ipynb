{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ef0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e68180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf6abf",
   "metadata": {},
   "source": [
    "# Initialisation de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9a6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Projet de Fin d'Etude\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-hadoop-cloud_2.12:3.3.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"bytebuffer\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.port\", \"9000\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37860e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051fff11",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4069db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations de connexion à RDS\n",
    "jdbc_url = \"jdbc:mysql://database-grp6.chcmvnn5gnpv.us-east-1.rds.amazonaws.com:3306/cleaned_data?serverTimezone=UTC\"\n",
    "db_properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"xxx\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd2bbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des noms des tables RDS\n",
    "table_names = [\n",
    "    \"orders\",\n",
    "    \"order_items\",\n",
    "    \"order_payments\",\n",
    "    \"order_reviews\",\n",
    "    \"products\",\n",
    "    \"products_translated\",\n",
    "    \"geolocation\",\n",
    "    \"states_name\",\n",
    "    \"customers\",\n",
    "    \"sellers\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8489eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orders': DataFrame[order_id: string, customer_id: string, order_status: string, order_purchase_timestamp: timestamp, order_approved_at: timestamp, order_delivered_carrier_date: timestamp, order_delivered_customer_date: timestamp, order_estimated_delivery_date: timestamp],\n",
       " 'order_items': DataFrame[order_id: string, order_item_id: int, product_id: string, seller_id: string, shipping_limit_date: timestamp, price: double, freight_value: double, total_items_value: double, total_freight_value: double, total_order_value: double],\n",
       " 'order_payments': DataFrame[order_id: string, payment_sequential: int, payment_type: string, payment_installments: int, payment_value: double],\n",
       " 'order_reviews': DataFrame[review_id: string, order_id: string, review_score: int, review_comment_title: string, review_comment_message: string, review_creation_date: timestamp, review_answer_timestamp: timestamp],\n",
       " 'products': DataFrame[product_id: string, product_category_name: string, product_name_length: int, product_description_length: int, product_photos_qty: int, product_weight_g: int, product_length_cm: int, product_height_cm: int, product_width_cm: int],\n",
       " 'products_translated': DataFrame[product_category_name: string, product_category_name_english: string],\n",
       " 'geolocation': DataFrame[geolocation_zip_code_prefix: int, geolocation_lat: double, geolocation_lng: double, geolocation_city: string, geolocation_state: string],\n",
       " 'states_name': DataFrame[geolocation_state: string, state_name: string],\n",
       " 'customers': DataFrame[customer_id: string, customer_unique_id: string, customer_zip_code: int, customer_city: string, customer_state: string],\n",
       " 'sellers': DataFrame[seller_id: string, seller_zip_code: int, seller_city: string, seller_state: string]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer un dictionnaire de DataFrames\n",
    "dataframes = {table_name: spark.read.jdbc(url=jdbc_url, table=table_name, properties=db_properties) for table_name in table_names}\n",
    "dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340df78d",
   "metadata": {},
   "source": [
    "# Analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ebc05a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- product_category_name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      " |-- total_items_value: double (nullable = true)\n",
      " |-- total_freight_value: double (nullable = true)\n",
      " |-- total_order_value: double (nullable = true)\n",
      " |-- product_description_length: integer (nullable = true)\n",
      " |-- product_photos_qty: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_purchase_year: integer (nullable = true)\n",
      " |-- order_purchase_month: integer (nullable = true)\n",
      " |-- order_purchase_day: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création d'un Dataframe global\n",
    "sales = dataframes[\"order_items\"].alias(\"i\") \\\n",
    "    .join(dataframes[\"products\"].alias(\"p\"), F.col(\"i.product_id\") == F.col(\"p.product_id\"), \"left\") \\\n",
    "    .join(dataframes[\"orders\"].alias(\"o\"), F.col(\"i.order_id\") == F.col(\"o.order_id\"), \"left\") \\\n",
    "    .join(dataframes[\"customers\"].alias(\"c\"), F.col(\"o.customer_id\") == F.col(\"c.customer_id\"), \"left\") \\\n",
    "    .join(dataframes[\"states_name\"].alias(\"s\"), F.col(\"c.customer_state\") == F.col(\"s.geolocation_state\"), \"left\") \\\n",
    "    .withColumn(\"order_purchase_year\", F.year(F.col(\"o.order_purchase_timestamp\"))) \\\n",
    "    .withColumn(\"order_purchase_month\", F.month(F.col(\"o.order_purchase_timestamp\"))) \\\n",
    "    .withColumn(\"order_purchase_day\", F.dayofmonth(F.col(\"o.order_purchase_timestamp\")))\n",
    "\n",
    "\n",
    "sales = sales.select(\"i.order_id\", \n",
    "                     \"i.product_id\",\n",
    "                     \"o.customer_id\",\n",
    "                     \"i.seller_id\", \n",
    "                     \"i.order_item_id\", \n",
    "                     \"p.product_category_name\", \n",
    "                     \"i.price\", \n",
    "                     \"i.freight_value\", \n",
    "                     \"i.total_items_value\", \n",
    "                     \"i.total_freight_value\", \n",
    "                     \"i.total_order_value\", \n",
    "                     \"p.product_description_length\",\n",
    "                     \"p.product_photos_qty\",\n",
    "                     \"o.order_status\",\n",
    "                     \"o.order_purchase_timestamp\",\n",
    "                     \"order_purchase_year\",\n",
    "                     \"order_purchase_month\",\n",
    "                     \"order_purchase_day\",\n",
    "                     \"c.customer_city\",\n",
    "                     \"s.state_name\")\n",
    "\n",
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06169af8",
   "metadata": {},
   "source": [
    "# Régression Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6269e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-----+----+\n",
      "|category|sales|year|month|date|\n",
      "+--------+-----+----+-----+----+\n",
      "+--------+-----+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regression linéaire : prévision des prix\n",
    "schema = StructType([\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"sales\", IntegerType(), False),\n",
    "    StructField(\"year\", IntegerType(), False),\n",
    "    StructField(\"month\", IntegerType(), False),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "price_predictions = spark.createDataFrame(sc.emptyRDD(), schema)\n",
    "\n",
    "price_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0ff335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération de toutes les catégories\n",
    "categories = sales.select(F.col(\"product_category_name\").alias(\"category\")).distinct().orderBy(F.col(\"product_category_name\").asc()).collect()\n",
    "\n",
    "# Sélectionner les colonnes pertinentes pour la régression linéaire\n",
    "sales_reg = sales.select(\n",
    "    F.col(\"order_purchase_year\").alias(\"year\"),\n",
    "    F.col(\"order_purchase_month\").alias(\"month\"),\n",
    "    F.col(\"product_category_name\").alias(\"category\"),\n",
    "    F.col(\"total_order_value\").alias(\"sales\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc43f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction permettant de réaliser la Régression linéaire\n",
    "def regression_lineaire(df_train, future_date, category) :\n",
    "    df_train = df_train.groupBy(\"year\", \"month\") \\\n",
    "        .agg(F.sum(\"sales\").alias(\"sales\"))\n",
    "    \n",
    "    # Assembler les fonctionnalités\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"year\", \"month\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    # Transformer les données en utilisant l'assembler\n",
    "    sales_data = assembler.transform(df_train)\n",
    "    sales_data = sales_data.select(\"features\", \"sales\")\n",
    "\n",
    "    # Créer le modèle de régression linéaire\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"sales\")\n",
    "\n",
    "    # Ajuster le modèle aux données d'entraînement\n",
    "    lr_model = lr.fit(sales_data)\n",
    "\n",
    "    # Transformer les données en utilisant l'assembler\n",
    "    df_predicted = spark.createDataFrame(future_date, [\"year\", \"month\"])\n",
    "    df_predicted = assembler.transform(df_predicted).select(\"features\")\n",
    "\n",
    "    # Prédire les prix pour les dates futures\n",
    "    df_predicted = lr_model.transform(df_predicted)\n",
    "\n",
    "    # Assemblage des DataFrames train et predicted en RDD\n",
    "    rdd = sc.parallelize(sales_data.union(df_predicted).rdd.collect()) \\\n",
    "        .map(lambda row: Row(features=row.features.toArray().tolist(), sales=row.sales)) \\\n",
    "        .map(lambda x: (int(x.features[0]), int(x.features[1]), x.sales))\n",
    "    \n",
    "    # Créer un DataFrame à partir de l'RDD avec les noms de colonnes appropriés\n",
    "    df = rdd.toDF([\"year\", \"month\", \"sales\"]) \\\n",
    "        .withColumn(\"category\", F.lit(category)) \\\n",
    "        .withColumn(\"date\", F.to_date(F.concat(F.col(\"year\"), F.lit(\"-\"), F.col(\"month\"), F.lit(\"-01\")))) \\\n",
    "        .select(\"category\", \"sales\", \"year\", \"month\", \"date\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6983a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----+-----+-------------------+\n",
      "|            category|             sales|year|month|               date|\n",
      "+--------------------+------------------+----+-----+-------------------+\n",
      "|agro_industry_and...|            110.69|2017|    3|2017-03-01 00:00:00|\n",
      "|agro_industry_and...|             224.6|2017|    8|2017-08-01 00:00:00|\n",
      "|agro_industry_and...|3558.4299999999994|2017|   10|2017-10-01 00:00:00|\n",
      "|agro_industry_and...|           6296.51|2018|    1|2018-01-01 00:00:00|\n",
      "|agro_industry_and...| 6567.089999999999|2018|    3|2018-03-01 00:00:00|\n",
      "|agro_industry_and...| 5543.740000000001|2018|    8|2018-08-01 00:00:00|\n",
      "|agro_industry_and...|           1199.47|2017|    7|2017-07-01 00:00:00|\n",
      "|agro_industry_and...|           3433.22|2018|    5|2018-05-01 00:00:00|\n",
      "|agro_industry_and...| 5664.040000000001|2017|   12|2017-12-01 00:00:00|\n",
      "|agro_industry_and...|           2119.69|2017|    9|2017-09-01 00:00:00|\n",
      "|agro_industry_and...| 8735.280000000002|2018|    7|2018-07-01 00:00:00|\n",
      "|agro_industry_and...|           3843.05|2018|    6|2018-06-01 00:00:00|\n",
      "|agro_industry_and...|331.09999999999997|2017|    2|2017-02-01 00:00:00|\n",
      "|agro_industry_and...|          14610.32|2017|   11|2017-11-01 00:00:00|\n",
      "|agro_industry_and...|1763.7400000000002|2017|    5|2017-05-01 00:00:00|\n",
      "|agro_industry_and...|           7879.24|2018|    2|2018-02-01 00:00:00|\n",
      "|agro_industry_and...|           1422.05|2017|    6|2017-06-01 00:00:00|\n",
      "|agro_industry_and...|            107.76|2017|    1|2017-01-01 00:00:00|\n",
      "|agro_industry_and...|           4783.66|2018|    4|2018-04-01 00:00:00|\n",
      "|agro_industry_and...| 8345.969399739057|2018|    9|2018-09-01 00:00:00|\n",
      "+--------------------+------------------+----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création d'une liste de listes contenant les dates à prédire\n",
    "future_date = [[2018,9], [2018, 10], [2018, 11], [2018, 12]] + \\\n",
    "    [[year, month] for year in range(2019, 2022) for month in range(1, 13)]\n",
    "\n",
    "\n",
    "# Prédictions des prix pour chaque catégorie\n",
    "for c in categories :\n",
    "    category = c.category\n",
    "\n",
    "    df_train = sales_reg.filter(F.col(\"category\") == category)\n",
    "    df = regression_lineaire(df_train, future_date, category)\n",
    "    price_predictions = price_predictions.union(df)\n",
    "\n",
    "\n",
    "price_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28737e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['health_beauty',\n",
       " 'watches_gifts',\n",
       " 'bed_bath_table',\n",
       " 'sports_leisure',\n",
       " 'housewares',\n",
       " 'computers_accessories',\n",
       " 'furniture_decor',\n",
       " 'auto',\n",
       " 'baby',\n",
       " 'telephony']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "\n",
    "# Récupération de la plus grande date de prédiction\n",
    "max_date_row = price_predictions.agg(F.max(\"date\")).head()\n",
    "max_date = max_date_row[0]\n",
    "\n",
    "# Récupération des n catégories ayant les meilleures ventes\n",
    "best_categories_predicted = price_predictions.select(\"category\") \\\n",
    "    .filter(F.col(\"date\") == max_date) \\\n",
    "    .orderBy(F.col(\"sales\").desc()) \\\n",
    "    .head(n)\n",
    "\n",
    "# Mise sous forme de liste\n",
    "best_categories_predicted = [c.category for c in best_categories_predicted]\n",
    "best_categories_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39e8e9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----+-----+-------------------+\n",
      "|category|             sales|year|month|               date|\n",
      "+--------+------------------+----+-----+-------------------+\n",
      "|    auto|16971.079999999994|2017|    3|2017-03-01 00:00:00|\n",
      "|    auto| 24000.62000000001|2017|    8|2017-08-01 00:00:00|\n",
      "|    auto|24917.620000000003|2017|   10|2017-10-01 00:00:00|\n",
      "|    auto| 41256.23000000002|2018|    1|2018-01-01 00:00:00|\n",
      "|    auto|52723.470000000045|2018|    3|2018-03-01 00:00:00|\n",
      "|    auto|52783.059999999976|2018|    8|2018-08-01 00:00:00|\n",
      "|    auto|16261.740000000002|2017|    7|2017-07-01 00:00:00|\n",
      "|    auto| 47979.92000000003|2018|    5|2018-05-01 00:00:00|\n",
      "|    auto|           2257.56|2016|   10|2016-10-01 00:00:00|\n",
      "|    auto|          44979.33|2017|   12|2017-12-01 00:00:00|\n",
      "|    auto|16940.350000000006|2017|    9|2017-09-01 00:00:00|\n",
      "|    auto|          17785.29|2017|    4|2017-04-01 00:00:00|\n",
      "|    auto| 50830.56999999997|2018|    7|2018-07-01 00:00:00|\n",
      "|    auto| 51816.65000000003|2018|    6|2018-06-01 00:00:00|\n",
      "|    auto|14973.890000000001|2017|    2|2017-02-01 00:00:00|\n",
      "|    auto| 40445.67000000003|2017|   11|2017-11-01 00:00:00|\n",
      "|    auto|21461.409999999996|2017|    5|2017-05-01 00:00:00|\n",
      "|    auto| 47824.17999999999|2018|    2|2018-02-01 00:00:00|\n",
      "|    auto| 33871.18999999998|2017|    6|2017-06-01 00:00:00|\n",
      "|    auto| 5706.410000000001|2017|    1|2017-01-01 00:00:00|\n",
      "+--------+------------------+----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création du dataframe avec uniquement les n meilleures catégories\n",
    "price_predictions_top_category = price_predictions \\\n",
    "    .filter(F.col(\"category\").isin(best_categories_predicted))\n",
    "    \n",
    "price_predictions_top_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7898c1",
   "metadata": {},
   "source": [
    "# Chargement des Dataframes en CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e735da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des Dataframes de prédiction de prix au dictionnaire de Dataframes\n",
    "dataframes[\"price_predictions\"] = price_predictions\n",
    "dataframes[\"price_predictions_top_category\"] = price_predictions_top_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c812a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:563: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if not is_datetime64tz_dtype(pser.dtype):\n",
      "C:\\Users\\cleme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\types.py:379: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(s.dtype):\n"
     ]
    }
   ],
   "source": [
    "# Configurer la connexion à S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Enregistrer les DataFrame en tant que CSV sur S3\n",
    "for table_name, df in dataframes.items() :\n",
    "    print(f\"Chargement du Dataframe {table_name}\")\n",
    "\n",
    "    # Transformer le Dataframe spark en Dataframe Pandas\n",
    "    pdf = df.toPandas()\n",
    "\n",
    "    # Créer une chaîne CSV à partir du DataFrame Pandas\n",
    "    csv_buffer = StringIO()\n",
    "    pdf.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    # Spécifier le chemin S3\n",
    "    s3_path = f\"output_data/{table_name}.csv\"\n",
    "\n",
    "    # Écrire le fichier CSV sur S3\n",
    "    s3.put_object(Bucket='grp6-pfe-data-engineering', Key=s3_path, Body=csv_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
